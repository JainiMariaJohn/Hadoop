{ coping files to read }
hadoop fs -copyFromLocal emp.csv pig/emp.csv

{ loading data }
DfA = load'/user/data/empdata_pig.csv' using PigStorage(',') as (id:int, name:chararray, sal:int, dpno:int);
describe DfA;
dump DfA;

{ aggregate }
DfB = filter DfA by sal>=100;
dump DfB;
DfC = filter DfA by empno==20 and sal==250;
dump DfC;

DfC= limit DfB 3;
DfD= order DfC by sal desc;

store DfB into '/user/pig/DfB' using PigStorage(',');

{ Transform (by column) }
DfE = foreach DfA generate id;
dump DfE;
{creating new column}
DfF= foreach DfA generate *, sal*5 as Incentive;
dump DfF;
{transforming columns}
DfG= foreach DfA generate SUBSTRING(ename,0,4);

{ Advanced codes }




